---
phase: 01-inference-engine
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - appliances/slm-copilot/appliance.sh
autonomous: true

must_haves:
  truths:
    - "service_install downloads the 14.3 GB GGUF model file to /opt/local-ai/models/"
    - "service_install starts LocalAI temporarily, sends a test inference request, and shuts it down (pre-warming)"
    - "service_configure generates model YAML with use_jinja: true, correct context_size, and threads from context variables"
    - "service_configure generates the systemd unit file binding to 127.0.0.1:8080 with Restart=on-failure"
    - "service_configure generates the environment file from context variables"
    - "service_bootstrap starts LocalAI via systemctl enable+start and waits for /readyz HTTP 200"
  artifacts:
    - path: "appliances/slm-copilot/appliance.sh"
      provides: "Complete service_install (with model + pre-warm), service_configure, and service_bootstrap"
      contains: "use_jinja"
      min_lines: 200
  key_links:
    - from: "service_configure"
      to: "/opt/local-ai/models/devstral-small-2.yaml"
      via: "cat heredoc overwrite"
      pattern: "cat.*>.*devstral-small-2.yaml"
    - from: "service_configure"
      to: "/etc/systemd/system/local-ai.service"
      via: "cat heredoc overwrite"
      pattern: "cat.*>.*local-ai.service"
    - from: "service_configure"
      to: "/opt/local-ai/config/local-ai.env"
      via: "cat heredoc overwrite"
      pattern: "cat.*>.*local-ai.env"
    - from: "service_bootstrap"
      to: "http://127.0.0.1:8080/readyz"
      via: "health check loop with curl"
      pattern: "curl.*readyz"
---

<objective>
Implement model download with build-time pre-warming in service_install(), complete service_configure() to generate all config files idempotently, and implement service_bootstrap() to start LocalAI and verify readiness.

Purpose: After this plan, the appliance script has all three lifecycle stages fully functional. A Packer build would produce an image where booting the VM starts LocalAI serving the Devstral model on localhost:8080.
Output: Complete appliance.sh with service_install (binary + model + pre-warm), service_configure (YAML + env + systemd), and service_bootstrap (start + health check).
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/01-inference-engine/01-RESEARCH.md
@.planning/phases/01-inference-engine/01-01-SUMMARY.md
@appliances/slm-copilot/appliance.sh
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add GGUF model download and build-time pre-warming to service_install</name>
  <files>appliances/slm-copilot/appliance.sh</files>
  <action>
Extend service_install() by appending the following steps AFTER the existing backend pre-install step (from plan 01-01). Do NOT remove or modify the existing steps (apt-get, user creation, directory creation, binary download, backend pre-install).

**Step: Download GGUF model** (INFER-03):
Add after the backend pre-install:
```bash
msg info "Downloading Devstral Small 2 Q4_K_M model (~14.3 GB)"
curl -fSL -C - -o "${LOCALAI_MODELS_DIR}/${LOCALAI_GGUF_FILE}" \
    "${LOCALAI_GGUF_URL}"
```
Use `-C -` for resume capability in case of network interruption during the 14 GB download. Use `-fSL` to fail on HTTP errors and follow redirects. Do NOT use `-s` -- show progress during Packer build.

Verify download size:
```bash
local _file_size
_file_size=$(stat -c%s "${LOCALAI_MODELS_DIR}/${LOCALAI_GGUF_FILE}")
if [ "${_file_size}" -lt 14000000000 ]; then
    msg error "GGUF file is only ${_file_size} bytes -- expected ~14.3 GB. Download may be corrupted."
    exit 1
fi
msg info "GGUF model downloaded ($((_file_size / 1073741824)) GB)"
```

**Step: Build-time pre-warming** (INFER-09 verification):
After the model download and before the ownership fix, add:
```bash
# Generate a minimal model YAML for pre-warming
cat > "${LOCALAI_MODELS_DIR}/${LOCALAI_MODEL_NAME}.yaml" <<YAML
name: ${LOCALAI_MODEL_NAME}
backend: llama-cpp
parameters:
  model: ${LOCALAI_GGUF_FILE}
context_size: 2048
threads: 2
mmap: true
mmlock: false
use_jinja: true
YAML

msg info "Pre-warming: starting LocalAI for build-time verification"
"${LOCALAI_BIN}" run \
    --address 127.0.0.1:8080 \
    --models-path "${LOCALAI_MODELS_DIR}" \
    --disable-webui &
local _prewarm_pid=$!

# Wait for readiness (model loading can take 60-180s on CPU)
local _elapsed=0
while ! curl -sf http://127.0.0.1:8080/readyz >/dev/null 2>&1; do
    sleep 5
    _elapsed=$((_elapsed + 5))
    if [ "${_elapsed}" -ge 300 ]; then
        msg error "Pre-warm: LocalAI not ready after 300s"
        kill "${_prewarm_pid}" 2>/dev/null || true
        wait "${_prewarm_pid}" 2>/dev/null || true
        exit 1
    fi
done
msg info "Pre-warm: LocalAI ready (${_elapsed}s)"

# Test inference
local _response
_response=$(curl -sf http://127.0.0.1:8080/v1/chat/completions \
    -H 'Content-Type: application/json' \
    -d "{\"model\":\"${LOCALAI_MODEL_NAME}\",\"messages\":[{\"role\":\"user\",\"content\":\"Say hello\"}],\"max_tokens\":5}")
echo "${_response}" | jq -e '.choices[0].message.content' >/dev/null 2>&1 || {
    msg error "Pre-warm smoke test failed: no content in response"
    kill "${_prewarm_pid}" 2>/dev/null || true
    wait "${_prewarm_pid}" 2>/dev/null || true
    exit 1
}
msg info "Pre-warm smoke test passed"

# Clean shutdown
kill "${_prewarm_pid}"
wait "${_prewarm_pid}" 2>/dev/null || true
msg info "Pre-warm: LocalAI shut down"

# Remove pre-warm model YAML (service_configure generates the real one)
rm -f "${LOCALAI_MODELS_DIR}/${LOCALAI_MODEL_NAME}.yaml"
```

**Then** apply the ownership fix (move the existing `chown -R` to AFTER the pre-warm, since the binary creates backend files owned by root):
```bash
chown -R "${LOCALAI_UID}:${LOCALAI_GID}" "${LOCALAI_BASE_DIR}"
```

**Important:** The pre-warm creates a temporary model YAML with minimal settings (context_size: 2048, threads: 2) just for verification. It is deleted after the smoke test. The real model YAML is generated by service_configure() with actual user-configured values.
  </action>
  <verify>
Run `bash -n appliances/slm-copilot/appliance.sh` to verify syntax.
Run `grep -c 'LOCALAI_GGUF_URL' appliances/slm-copilot/appliance.sh` -- must find the download curl command.
Run `grep -c 'Pre-warming\|Pre-warm' appliances/slm-copilot/appliance.sh` -- must be >= 3 (start, test, shutdown messages).
Run `grep 'use_jinja' appliances/slm-copilot/appliance.sh` -- must appear in the pre-warm YAML.
Run `grep 'rm -f.*yaml' appliances/slm-copilot/appliance.sh` -- must show cleanup of pre-warm YAML.
  </verify>
  <done>service_install downloads the 14.3 GB GGUF to /opt/local-ai/models/, runs a full pre-warm cycle (start LocalAI, wait for readyz, test inference, verify response, shut down), cleans up temporary YAML, and fixes ownership.</done>
</task>

<task type="auto">
  <name>Task 2: Implement service_configure and service_bootstrap with config generation and health check</name>
  <files>appliances/slm-copilot/appliance.sh</files>
  <action>
Replace the service_configure() and service_bootstrap() stubs with full implementations.

**service_configure() implementation:**

This function runs on EVERY boot. All operations MUST be idempotent (use `>` overwrite, never `>>` append).

```bash
service_configure() {
    msg info "Configuring SLM-Copilot"

    # 1. Ensure directory structure exists (idempotent)
    mkdir -p "${LOCALAI_MODELS_DIR}" "${LOCALAI_CONFIG_DIR}"

    # 2. Check for AVX2 support (warn only, don't fail)
    if ! grep -q avx2 /proc/cpuinfo; then
        msg warning "CPU does not support AVX2 -- LocalAI inference may fail (SIGILL) or be very slow"
    fi

    # 3. Generate model YAML (INFER-01, INFER-06, INFER-07)
    #    CRITICAL: use_jinja must be true for Devstral chat template
    generate_model_yaml

    # 4. Generate environment file
    generate_env_file

    # 5. Generate systemd unit file (INFER-04, INFER-08)
    generate_systemd_unit

    # 6. Reload systemd
    systemctl daemon-reload

    msg info "SLM-Copilot configuration complete"
}
```

**Helper function: generate_model_yaml()**
```bash
generate_model_yaml() {
    cat > "${LOCALAI_MODELS_DIR}/${LOCALAI_MODEL_NAME}.yaml" <<YAML
# Devstral Small 2 24B (Q4_K_M) -- generated at $(date -u +"%Y-%m-%dT%H:%M:%SZ")
name: ${LOCALAI_MODEL_NAME}
backend: llama-cpp
parameters:
  model: ${LOCALAI_GGUF_FILE}
  temperature: 0.15
  top_p: 0.95
context_size: ${ONEAPP_COPILOT_CONTEXT_SIZE}
threads: ${ONEAPP_COPILOT_THREADS}
mmap: true
mmlock: false
use_jinja: true
YAML
    chown "${LOCALAI_UID}:${LOCALAI_GID}" "${LOCALAI_MODELS_DIR}/${LOCALAI_MODEL_NAME}.yaml"
    msg info "Model YAML written to ${LOCALAI_MODELS_DIR}/${LOCALAI_MODEL_NAME}.yaml (context_size=${ONEAPP_COPILOT_CONTEXT_SIZE}, threads=${ONEAPP_COPILOT_THREADS})"
}
```

**Helper function: generate_env_file()**
```bash
generate_env_file() {
    cat > "${LOCALAI_ENV_FILE}" <<EOF
# LocalAI environment -- generated at $(date -u +"%Y-%m-%dT%H:%M:%SZ")
LOCALAI_THREADS=${ONEAPP_COPILOT_THREADS}
LOCALAI_CONTEXT_SIZE=${ONEAPP_COPILOT_CONTEXT_SIZE}
LOCALAI_LOG_LEVEL=info
EOF
    chmod 0640 "${LOCALAI_ENV_FILE}"
    msg info "Environment file written to ${LOCALAI_ENV_FILE}"
}
```

**Helper function: generate_systemd_unit()**
Write the systemd unit per research findings. Key points:
- `User=localai`, `Group=localai`
- `EnvironmentFile` pointing to the env file
- `ExecStart` with `--address 127.0.0.1:8080` (INFER-08), `--models-path`, `--disable-webui`
- `Restart=on-failure`, `RestartSec=10` (INFER-04)
- `TimeoutStartSec=300` (model loading takes 30-120s)
- `LimitNOFILE=65536`
- `OOMScoreAdjust=-500` (protect from OOM killer)
- `After=network-online.target`, `Wants=network-online.target`
- `WantedBy=multi-user.target`

```bash
generate_systemd_unit() {
    cat > "${LOCALAI_SYSTEMD_UNIT}" <<EOF
[Unit]
Description=LocalAI LLM Inference Server (SLM-Copilot)
After=network-online.target
Wants=network-online.target

[Service]
Type=simple
User=localai
Group=localai
EnvironmentFile=${LOCALAI_ENV_FILE}
ExecStart=${LOCALAI_BIN} run \\
    --address 127.0.0.1:8080 \\
    --models-path ${LOCALAI_MODELS_DIR} \\
    --disable-webui
Restart=on-failure
RestartSec=10
TimeoutStartSec=300
LimitNOFILE=65536
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
EOF
    msg info "Systemd unit written to ${LOCALAI_SYSTEMD_UNIT}"
}
```

**service_bootstrap() implementation:**

```bash
service_bootstrap() {
    msg info "Bootstrapping SLM-Copilot"

    # 1. Enable and start LocalAI
    systemctl enable local-ai.service
    systemctl start local-ai.service

    # 2. Wait for readiness (INFER-05)
    wait_for_localai

    msg info "SLM-Copilot bootstrap complete -- LocalAI serving on 127.0.0.1:8080"
}
```

**Helper function: wait_for_localai()**
```bash
wait_for_localai() {
    local _timeout=300
    local _elapsed=0
    msg info "Waiting for LocalAI readiness (timeout: ${_timeout}s)"
    while ! curl -sf http://127.0.0.1:8080/readyz >/dev/null 2>&1; do
        sleep 5
        _elapsed=$((_elapsed + 5))
        if [ "${_elapsed}" -ge "${_timeout}" ]; then
            msg error "LocalAI not ready after ${_timeout}s -- check: journalctl -u local-ai"
            exit 1
        fi
    done
    msg info "LocalAI ready (${_elapsed}s)"
}
```

**Place all helper functions** (generate_model_yaml, generate_env_file, generate_systemd_unit, wait_for_localai) in a clearly commented HELPER section below service_help(), following the SuperLink pattern.

**Important implementation notes:**
- All `cat > file <<EOF` blocks use `>` (overwrite), never `>>` (append) -- idempotency requirement
- The systemd unit uses `\\` for line continuations inside the heredoc (the shell expands variables but preserves backslashes when using `<<EOF` not `<<'EOF'`)
- `systemctl daemon-reload` is in service_configure, NOT in service_bootstrap -- configure writes the unit, bootstrap starts it
  </action>
  <verify>
Run `bash -n appliances/slm-copilot/appliance.sh` to verify syntax.
Run `grep -c 'generate_model_yaml\|generate_env_file\|generate_systemd_unit\|wait_for_localai' appliances/slm-copilot/appliance.sh` -- must be >= 8 (4 definitions + 4 calls).
Run `grep 'use_jinja: true' appliances/slm-copilot/appliance.sh` to confirm it appears in the model YAML heredoc.
Run `grep '127.0.0.1:8080' appliances/slm-copilot/appliance.sh` -- must appear in systemd ExecStart (loopback binding, INFER-08).
Run `grep 'Restart=on-failure' appliances/slm-copilot/appliance.sh` -- systemd auto-restart (INFER-04).
Run `grep 'readyz' appliances/slm-copilot/appliance.sh` -- health check (INFER-05).
Run `grep 'daemon-reload' appliances/slm-copilot/appliance.sh` -- must be in service_configure.
Run `grep 'systemctl enable\|systemctl start' appliances/slm-copilot/appliance.sh` -- must be in service_bootstrap.
  </verify>
  <done>service_configure generates model YAML (with use_jinja: true, context_size, threads), env file (LOCALAI_THREADS, LOCALAI_CONTEXT_SIZE), and systemd unit (127.0.0.1:8080, Restart=on-failure, TimeoutStartSec=300). service_bootstrap enables+starts the service and waits for /readyz 200. All config generation is idempotent (overwrite, not append). Script passes bash -n.</done>
</task>

</tasks>

<verification>
- `bash -n appliances/slm-copilot/appliance.sh` exits 0
- service_install: downloads GGUF model (~14.3 GB), runs pre-warm cycle (start, test, shutdown), cleans up temp YAML
- service_configure: generates model YAML with use_jinja: true, env file with threads/context_size, systemd unit with 127.0.0.1:8080 binding
- service_bootstrap: systemctl enable + start, wait for /readyz 200
- All helper functions defined and called correctly
- All heredocs use `>` overwrite (idempotent)
- No hardcoded values -- all reference constants from the header
</verification>

<success_criteria>
The appliance.sh script has complete service_install (binary + model + pre-warm), service_configure (YAML + env + systemd), and service_bootstrap (start + health check) implementations. A Packer build using this script would produce an image where the VM boots, configures LocalAI from context variables, and serves the Devstral model on localhost:8080 with auto-restart and health checking.
</success_criteria>

<output>
After completion, create `.planning/phases/01-inference-engine/01-02-SUMMARY.md`
</output>
