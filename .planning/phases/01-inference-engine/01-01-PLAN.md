---
phase: 01-inference-engine
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - appliances/slm-copilot/appliance.sh
autonomous: true

must_haves:
  truths:
    - "appliance.sh exists with one-apps lifecycle skeleton (service_install, service_configure, service_bootstrap, service_cleanup, service_help)"
    - "service_install downloads LocalAI v3.11.0 binary to /opt/local-ai/bin/local-ai and makes it executable"
    - "service_install pre-installs the llama-cpp backend via local-ai backends install"
    - "service_install creates the localai system user and directory structure"
    - "service_install installs jq as a runtime dependency"
  artifacts:
    - path: "appliances/slm-copilot/appliance.sh"
      provides: "One-apps appliance lifecycle script with service_install fully implemented"
      contains: "service_install"
      min_lines: 80
  key_links:
    - from: "appliances/slm-copilot/appliance.sh"
      to: "ONE_SERVICE_PARAMS"
      via: "flat array definition in script header"
      pattern: "ONE_SERVICE_PARAMS="
    - from: "service_install"
      to: "/opt/local-ai/bin/local-ai"
      via: "curl download from GitHub releases"
      pattern: "curl.*LocalAI/releases"
---

<objective>
Create the appliance.sh skeleton following one-apps conventions and implement service_install() to download and set up the LocalAI binary with its llama-cpp backend.

Purpose: Establish the foundational appliance script that subsequent plans build upon. The install stage runs once during Packer build and must leave the system ready for model download and configuration.
Output: A working appliance.sh with complete service_install() and stub functions for configure/bootstrap/cleanup/help.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/01-inference-engine/01-RESEARCH.md
@appliances/slm-copilot/appliance.sh
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create appliance.sh skeleton with ONE_SERVICE_PARAMS and all lifecycle stubs</name>
  <files>appliances/slm-copilot/appliance.sh</files>
  <action>
Create `appliances/slm-copilot/appliance.sh` following the proven pattern from the flower-opennebula SuperLink appliance (`/home/pablo/flower-opennebula/appliances/flower_service/appliance-superlink.sh`).

The script MUST start with `#!/usr/bin/env bash` and include the file header comment block describing the SLM-Copilot appliance.

**Header section** (service metadata):
```
ONE_SERVICE_NAME='Service SLM-Copilot - Sovereign AI Coding Assistant'
ONE_SERVICE_VERSION='1.0.0'
ONE_SERVICE_BUILD=$(date +%s)
ONE_SERVICE_SHORT_DESCRIPTION='CPU-only AI coding copilot (Devstral Small 2 24B)'
ONE_SERVICE_DESCRIPTION='Sovereign AI coding assistant serving Devstral Small 2 24B
via LocalAI. OpenAI-compatible API for Cline/VS Code integration.
CPU-only inference, no GPU required.'
ONE_SERVICE_RECONFIGURABLE=true
```

**ONE_SERVICE_PARAMS array** (4-element stride: 'VARNAME' 'lifecycle_step' 'Description' 'default_value'):
- `ONEAPP_COPILOT_CONTEXT_SIZE` / `configure` / `Model context window in tokens` / `32768`
- `ONEAPP_COPILOT_THREADS` / `configure` / `CPU threads for inference (0=auto-detect)` / `0`

**Default value assignments** (with `${VAR:-default}` pattern):
```bash
ONEAPP_COPILOT_CONTEXT_SIZE="${ONEAPP_COPILOT_CONTEXT_SIZE:-32768}"
ONEAPP_COPILOT_THREADS="${ONEAPP_COPILOT_THREADS:-0}"
```

**Constants section:**
```bash
readonly LOCALAI_VERSION="3.11.0"
readonly LOCALAI_BASE_DIR="/opt/local-ai"
readonly LOCALAI_BIN="${LOCALAI_BASE_DIR}/bin/local-ai"
readonly LOCALAI_MODELS_DIR="${LOCALAI_BASE_DIR}/models"
readonly LOCALAI_CONFIG_DIR="${LOCALAI_BASE_DIR}/config"
readonly LOCALAI_ENV_FILE="${LOCALAI_CONFIG_DIR}/local-ai.env"
readonly LOCALAI_SYSTEMD_UNIT="/etc/systemd/system/local-ai.service"
readonly LOCALAI_MODEL_NAME="devstral-small-2"
readonly LOCALAI_GGUF_FILE="devstral-small-2-q4km.gguf"
readonly LOCALAI_GGUF_URL="https://huggingface.co/bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/mistralai_Devstral-Small-2-24B-Instruct-2512-Q4_K_M.gguf"
readonly LOCALAI_UID=49999
readonly LOCALAI_GID=49999
```

**Lifecycle stubs:**
- `service_configure()` -- stub with `msg info "Configuring SLM-Copilot"` and a comment `# Implemented in plan 01-03`
- `service_bootstrap()` -- stub with `msg info "Bootstrapping SLM-Copilot"` and comment `# Implemented in plan 01-02`
- `service_cleanup()` -- no-op (`:`) with comment explaining why (same pattern as SuperLink)
- `service_help()` -- basic heredoc with service name, key vars, ports, and service management commands

Do NOT implement service_install yet -- that is Task 2. Just create the stub: `service_install() { msg info "Installing SLM-Copilot appliance components" ; }` with a comment.
  </action>
  <verify>
Run `bash -n appliances/slm-copilot/appliance.sh` to verify syntax.
Run `grep -c 'service_install\|service_configure\|service_bootstrap\|service_cleanup\|service_help' appliances/slm-copilot/appliance.sh` and confirm all 5 lifecycle functions are present (count >= 5).
Run `grep 'ONE_SERVICE_PARAMS' appliances/slm-copilot/appliance.sh` to confirm the params array exists.
  </verify>
  <done>appliance.sh exists with valid bash syntax, all 5 lifecycle functions defined, ONE_SERVICE_PARAMS array with CONTEXT_SIZE and THREADS, constants section with all paths/versions, and default value assignments.</done>
</task>

<task type="auto">
  <name>Task 2: Implement service_install with LocalAI binary download, backend pre-install, and directory setup</name>
  <files>appliances/slm-copilot/appliance.sh</files>
  <action>
Replace the service_install() stub with the full implementation. The function runs once during Packer build time.

**Steps in order:**

1. **Install runtime dependencies:**
```bash
export DEBIAN_FRONTEND=noninteractive
apt-get update -qq
apt-get install -y -qq jq >/dev/null
```
Only `jq` is needed for Phase 1 (JSON parsing for health checks). No Docker, no Nginx.

2. **Create system user and group:**
```bash
groupadd --system --gid "${LOCALAI_GID}" localai 2>/dev/null || true
useradd --system --uid "${LOCALAI_UID}" --gid "${LOCALAI_GID}" \
    --home-dir "${LOCALAI_BASE_DIR}" --shell /usr/sbin/nologin localai 2>/dev/null || true
```

3. **Create directory structure:**
```bash
mkdir -p "${LOCALAI_BASE_DIR}/bin" \
         "${LOCALAI_MODELS_DIR}" \
         "${LOCALAI_CONFIG_DIR}"
```

4. **Download LocalAI binary:**
```bash
msg info "Downloading LocalAI v${LOCALAI_VERSION} binary"
curl -fSL -o "${LOCALAI_BIN}" \
    "https://github.com/mudler/LocalAI/releases/download/v${LOCALAI_VERSION}/local-ai-Linux-x86_64"
chmod +x "${LOCALAI_BIN}"
```
Use `-fSL` (fail on HTTP errors, show errors, follow redirects). Do NOT use `-s` -- we want progress output during Packer build.

5. **Pre-install llama-cpp backend** (INFER-09):
```bash
msg info "Pre-installing llama-cpp backend"
"${LOCALAI_BIN}" backends install llama-cpp
```

6. **Set ownership:**
```bash
chown -R "${LOCALAI_UID}:${LOCALAI_GID}" "${LOCALAI_BASE_DIR}"
```

7. **Log completion:**
```bash
msg info "SLM-Copilot appliance install complete (LocalAI v${LOCALAI_VERSION})"
```

**Important notes:**
- Do NOT download the GGUF model in this task. That is plan 01-02.
- Do NOT create systemd unit or model YAML. That is plan 01-02.
- Do NOT do pre-warming. That is plan 01-02.
- The function must be idempotent -- `mkdir -p`, `2>/dev/null || true` for user creation.
  </action>
  <verify>
Run `bash -n appliances/slm-copilot/appliance.sh` to verify syntax.
Run `grep -c 'curl.*LocalAI/releases' appliances/slm-copilot/appliance.sh` -- must be 1.
Run `grep -c 'backends install llama-cpp' appliances/slm-copilot/appliance.sh` -- must be 1.
Run `grep 'LOCALAI_UID\|LOCALAI_GID' appliances/slm-copilot/appliance.sh` to confirm user/group creation references the constants.
Run `grep 'apt-get install.*jq' appliances/slm-copilot/appliance.sh` to confirm jq installed.
  </verify>
  <done>service_install() downloads LocalAI v3.11.0 binary from GitHub releases, pre-installs llama-cpp backend, creates localai system user (UID 49999), creates /opt/local-ai/{bin,models,config} directory structure, installs jq. Script passes bash -n syntax check.</done>
</task>

</tasks>

<verification>
- `bash -n appliances/slm-copilot/appliance.sh` exits 0
- All 5 lifecycle functions present (service_install, service_configure, service_bootstrap, service_cleanup, service_help)
- ONE_SERVICE_PARAMS array defined with ONEAPP_COPILOT_CONTEXT_SIZE and ONEAPP_COPILOT_THREADS
- Constants section defines LOCALAI_VERSION, all paths, GGUF URL, UID/GID
- service_install contains: apt-get jq, user/group creation, directory creation, binary download, backend pre-install, ownership fix
- No model download, no systemd unit, no model YAML, no pre-warming in this plan
</verification>

<success_criteria>
appliance.sh exists at appliances/slm-copilot/appliance.sh with valid bash syntax, complete one-apps skeleton, and a fully implemented service_install() that prepares the system for model installation in plan 01-02.
</success_criteria>

<output>
After completion, create `.planning/phases/01-inference-engine/01-01-SUMMARY.md`
</output>
