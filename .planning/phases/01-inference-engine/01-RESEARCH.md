# Phase 1: Inference Engine - Research

**Researched:** 2026-02-14
**Domain:** LocalAI bare-metal installation, GGUF model configuration, systemd service management, OpenNebula one-apps appliance lifecycle
**Confidence:** HIGH

## Summary

Phase 1 delivers the core inference engine: LocalAI serving Devstral Small 2 24B (Q4_K_M) on CPU with streaming, health checks, configurable threads/context, and systemd lifecycle management. This is the foundation everything else builds on -- no Nginx, no TLS, no external access. Just LocalAI on localhost:8080 working correctly.

The critical finding is that LocalAI v3.11.0 (Feb 7, 2026) uses externalized backends -- the llama-cpp backend is NOT bundled in the binary. It must be explicitly pre-installed during the Packer build via `local-ai backends install llama-cpp`, or it will be auto-downloaded on first inference request (adding ~200 MB download delay). Additionally, the Devstral Small 2 model requires `use_jinja: true` in the model YAML to correctly apply its embedded chat template from the GGUF metadata -- without this, chat formatting will be broken.

**Primary recommendation:** Install LocalAI v3.11.0 binary, pre-install the llama-cpp backend via CLI, download the 14.3 GB GGUF during build, configure model YAML with `use_jinja: true` and explicit `context_size: 32768`, and verify the full stack with a test inference during `service_install()`.

## Standard Stack

### Core

| Component | Version | Purpose | Why Standard |
|-----------|---------|---------|--------------|
| LocalAI binary | v3.11.0 | LLM inference server, OpenAI-compatible API | Latest stable (Feb 7, 2026); single binary; native GGUF/llama.cpp support; built-in `/readyz` health check |
| Devstral Small 2 24B Q4_K_M | Instruct-2512 | Coding-specialized SLM | 68% SWE-bench Verified; Apache 2.0; 24B params fits CPU; Mistral partnership with Cline |
| llama-cpp backend | (bundled with LocalAI) | GGUF inference engine | Standard backend for GGUF models in LocalAI; externalized since v3.x |
| systemd | Ubuntu 24.04 | Process management | Standard Linux service management; auto-restart, boot ordering, journal logging |

### Supporting

| Tool | Version | Purpose | When to Use |
|------|---------|---------|-------------|
| curl | 8.5.x (Ubuntu repo) | Model download from HuggingFace during build | Packer build phase only |
| jq | 1.7.x (Ubuntu repo) | JSON parsing for health checks | Runtime health verification in bootstrap |

### Alternatives Considered

| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| LocalAI binary | Ollama | Simpler but less configurable; no per-model YAML; harder to tune context_size/threads independently |
| LocalAI binary | Raw llama-server | No model management, no OpenAI function calling, no YAML config, no gallery |
| Q4_K_M | Q5_K_M | 16.8 GB needs 24+ GB RAM with context; marginal quality gain (<1%) |
| Q4_K_M | Q3_K_M | 11.5 GB saves 3 GB but measurable quality drop |

**Installation (during Packer build / service_install):**
```bash
# 1. Download LocalAI binary
LOCALAI_VERSION="3.11.0"
curl -Lo /opt/local-ai/bin/local-ai \
  "https://github.com/mudler/LocalAI/releases/download/v${LOCALAI_VERSION}/local-ai-Linux-x86_64"
chmod +x /opt/local-ai/bin/local-ai

# 2. Pre-install llama-cpp backend (avoids first-request download)
/opt/local-ai/bin/local-ai backends install llama-cpp

# 3. Download GGUF model (~14.3 GB)
curl -Lo /opt/local-ai/models/devstral-small-2-q4km.gguf \
  "https://huggingface.co/bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/mistralai_Devstral-Small-2-24B-Instruct-2512-Q4_K_M.gguf"
```

## Architecture Patterns

### Recommended Directory Structure
```
/opt/local-ai/
  bin/
    local-ai                          # Pre-built binary (~50 MB)
  models/
    devstral-small-2.yaml             # Model config (generated by configure)
    devstral-small-2-q4km.gguf        # GGUF weights (~14.3 GB, baked in)
  config/
    local-ai.env                      # Environment file for systemd

/etc/systemd/system/
  local-ai.service                    # Systemd unit (generated by configure)

/etc/one-appliance/
  service.d/
    appliance.sh                      # Appliance lifecycle script
```

### Pattern 1: Three-Stage Appliance Lifecycle (one-apps)
**What:** Separate install (build-time), configure (every boot), bootstrap (first boot + start services) into distinct functions.
**When to use:** Always -- this is the one-apps framework convention.

```bash
# service_install() -- runs once during Packer build
#   - Download LocalAI binary
#   - Pre-install llama-cpp backend
#   - Download GGUF model
#   - Create directory structure + system user
#   - Pre-warm: run test inference to verify everything works
#   - Stop services (layers persist on disk)

# service_configure() -- runs EVERY boot (must be idempotent)
#   - Read ONEAPP_COPILOT_* context vars with defaults
#   - Generate model YAML from context vars (overwrite, never append)
#   - Generate systemd environment file
#   - Generate systemd unit file
#   - systemctl daemon-reload

# service_bootstrap() -- runs after configure
#   - Start LocalAI systemd service
#   - Wait for /readyz to return 200
#   - (Later phases: start Nginx, write report file)
```

### Pattern 2: Backend Pre-Warming During Build
**What:** During Packer build, after installing LocalAI, the model, and the backend, start LocalAI temporarily, send a test request, then shut down. This verifies the entire stack works at build time.
**When to use:** Always -- catches model corruption, missing backends, and YAML config errors at build time instead of at deployment.

```bash
# In service_install(), after model download and backend install:
/opt/local-ai/bin/local-ai run \
    --address 127.0.0.1:8080 \
    --models-path /opt/local-ai/models \
    --threads 2 &
LOCALAI_PID=$!

# Wait for readiness (model loading can take 60-120s on CPU)
for i in $(seq 1 120); do
    if curl -sf http://127.0.0.1:8080/readyz >/dev/null 2>&1; then
        break
    fi
    sleep 2
done

# Test inference (verifies model loads + backend works)
curl -sf http://127.0.0.1:8080/v1/chat/completions \
    -H 'Content-Type: application/json' \
    -d '{"model":"devstral-small-2","messages":[{"role":"user","content":"Say hello"}],"max_tokens":5}'

# Clean shutdown
kill "$LOCALAI_PID"
wait "$LOCALAI_PID" 2>/dev/null || true
```

### Pattern 3: Loopback-Only Binding
**What:** Bind LocalAI to `127.0.0.1:8080`, never to `0.0.0.0`.
**When to use:** Always. This is INFER-08.

```bash
# In systemd unit or CLI:
ExecStart=/opt/local-ai/bin/local-ai run --address 127.0.0.1:8080 ...
```

**Verification:** `ss -tlnp | grep 8080` should show `127.0.0.1:8080` only, never `0.0.0.0:8080` or `*:8080`.

### Pattern 4: Model YAML with use_jinja (CRITICAL)
**What:** Enable Jinja2 template processing from GGUF metadata for proper chat formatting.
**When to use:** Always for Devstral Small 2 (and any modern Mistral model).

```yaml
# /opt/local-ai/models/devstral-small-2.yaml
name: devstral-small-2
backend: llama-cpp
parameters:
  model: devstral-small-2-q4km.gguf
  temperature: 0.15
  top_p: 0.95
context_size: 32768
threads: 0           # 0 = auto-detect (nproc)
mmap: true
mmlock: false
use_jinja: true      # CRITICAL: Use chat template from GGUF metadata
```

**Why `use_jinja: true`:** Devstral Small 2 embeds its chat template in the GGUF metadata (Mistral V3 tokenizer format). Without this flag, LocalAI will not apply the correct chat template, resulting in garbled or nonsensical responses. This feature was merged in LocalAI PR #7120 (November 2025) and is available in v3.11.0.

### Pattern 5: Environment File for systemd
**What:** Use `EnvironmentFile` in the systemd unit to keep configurable values separate from the unit file itself.
**When to use:** Always -- enables `service_configure()` to update runtime values without rewriting the unit file.

```ini
# /opt/local-ai/config/local-ai.env
LOCALAI_THREADS=0
LOCALAI_CONTEXT_SIZE=32768
LOCALAI_LOG_LEVEL=info
```

```ini
# /etc/systemd/system/local-ai.service
[Service]
EnvironmentFile=/opt/local-ai/config/local-ai.env
ExecStart=/opt/local-ai/bin/local-ai run \
    --address 127.0.0.1:8080 \
    --models-path /opt/local-ai/models \
    --disable-webui
```

Note: Environment variables take precedence over CLI flags in LocalAI. The env file sets `LOCALAI_THREADS`, `LOCALAI_CONTEXT_SIZE`, etc. These override the model YAML defaults at runtime, which is exactly what we want for context variable-driven configuration.

### Anti-Patterns to Avoid
- **Binding to 0.0.0.0:** Exposes LocalAI directly to network. No TLS, no auth. Always use 127.0.0.1.
- **Using `backend: llama` instead of `backend: llama-cpp`:** `llama` is deprecated/non-existent. Always use `llama-cpp`.
- **Omitting `use_jinja: true`:** Model will not apply its embedded chat template. Chat responses will be garbled.
- **Setting context_size to 128K without RAM check:** KV cache for 128K context costs ~4-8 GB extra RAM. Default to 32768 (32K).
- **Using `>>` (append) in service_configure:** Must use `>` (overwrite) because configure runs every boot. Append creates duplicate entries.
- **Skipping backend pre-install:** First inference request triggers ~200 MB backend download, adding minutes of delay.

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| OpenAI-compatible API | Custom HTTP server wrapping llama.cpp | LocalAI | Handles SSE streaming, model management, health checks, token counting |
| Chat template formatting | Custom prompt engineering in bash | `use_jinja: true` in model YAML | GGUF metadata contains the official chat template; manual templates will drift |
| Process management | Custom PID tracking, cron restart | systemd with `Restart=on-failure` | Battle-tested, journal logging, boot ordering, watchdog support |
| Health checking | Custom curl loop in crontab | LocalAI `/readyz` endpoint | Built-in, returns 200 only when model is loaded and ready |
| Backend installation | Manual download of llama-cpp-grpc binary | `local-ai backends install llama-cpp` | Handles version matching, GPU detection, correct binary variant |

**Key insight:** LocalAI v3.x has externalized all backends. The binary itself is ~50 MB and lightweight. Backends are separate executables downloaded on demand or via CLI. Never manually download backend binaries -- use the `local-ai backends install` CLI command, which handles version matching.

## Common Pitfalls

### Pitfall 1: LocalAI Default context_size=512 Silently Truncates
**What goes wrong:** LocalAI defaults to `context_size: 512` tokens. Coding assistant system prompts alone consume 200-500 tokens, leaving almost no room for conversation. Responses become garbled after 1-2 turns with no API-level error.
**Why it happens:** Conservative safety default to prevent OOM on small systems.
**How to avoid:** ALWAYS set `context_size` explicitly in model YAML. For Devstral with 32 GB VM RAM: use 32768 (32K). For 64 GB: can go up to 65536.
**Warning signs:** Responses become short/repetitive/nonsensical after 2-3 conversation turns. No errors in logs.

### Pitfall 2: Missing llama-cpp Backend on First Request
**What goes wrong:** Binary-only install of LocalAI v3.x does not include the llama-cpp backend. First inference request triggers a ~200 MB download, adding 1-5 minutes delay. In air-gapped environments, it fails entirely.
**Why it happens:** Since mid-2025, LocalAI externalized all backends. The binary is a shell; backends are separate executables.
**How to avoid:** Run `local-ai backends install llama-cpp` during `service_install()`. Verify with `local-ai backends list`.
**Warning signs:** First request hangs for minutes then succeeds. Logs show "downloading backend" messages. `ls /tmp/localai/backend_data/backend-assets/grpc/` is empty before first request.

### Pitfall 3: CPU Instruction Set Mismatch (SIGILL)
**What goes wrong:** LocalAI binary compiled for AVX2 crashes with SIGILL on CPUs without AVX2 support. In VMs, this depends on the QEMU CPU model (`host` passthrough vs `qemu64`).
**Why it happens:** Packer builds with `-cpu host` expose build host's AVX2. Production VMs may use different CPU models.
**How to avoid:** Document that VM template must use `CPU_MODEL="host"` or a model supporting AVX2. Add runtime check in `service_configure()`:
```bash
if ! grep -q avx2 /proc/cpuinfo; then
    msg warning "CPU does not support AVX2 -- inference may fail or be slow"
fi
```
**Warning signs:** `local-ai` exits immediately with signal 4. No log output at all.

### Pitfall 4: GGUF Download Fails Mid-Transfer (14 GB)
**What goes wrong:** 14 GB download from HuggingFace CDN can fail due to network timeouts, rate limiting, or Packer SSH session timeout. Partial download wastes the entire build.
**Why it happens:** HuggingFace CDN can be unstable for large files. Packer provisioner timeouts may kill long-running downloads.
**How to avoid:**
1. Use `curl -C -` (resume-capable) or `wget -c` for downloads
2. Set Packer `ssh_timeout` to at least 30m
3. Verify file size after download matches expected (~14.3 GB)
4. Consider pre-downloading to Packer host and injecting via `file` provisioner
**Warning signs:** Packer build hangs >20 min at shell provisioner then fails. Model file size doesn't match expected.

### Pitfall 5: service_configure Non-Idempotent Breaks on Reboot
**What goes wrong:** Operations that append to files, create duplicate systemd units, or aren't idempotent break when `service_configure` runs on the second boot.
**Why it happens:** one-apps calls `service_configure` on EVERY boot for reconfigurability.
**How to avoid:** Every operation must be idempotent. Use `>` (overwrite) not `>>` (append). Use `mkdir -p` not `mkdir`. Always overwrite YAML + env file + unit file entirely. Always `systemctl daemon-reload` after writing unit file.
**Warning signs:** Duplicate entries in config files after reboot. Services fail on second boot but worked on first.

### Pitfall 6: Model Loading Takes 30-120 Seconds on CPU
**What goes wrong:** After LocalAI starts, the model takes 30-120 seconds to load into memory via mmap. During this time, `/readyz` returns non-200 and API requests fail with 500 errors.
**Why it happens:** 14 GB file must be memory-mapped and the KV cache initialized. CPU-only systems are slow at this.
**How to avoid:** In `service_bootstrap()`, implement a health check loop that waits for `/readyz` to return 200 before declaring the service ready. Use `TimeoutStartSec=300` in the systemd unit.
**Warning signs:** Service appears "started" (systemd shows active) but API returns errors. `/readyz` returns 503.

## Code Examples

### Model YAML Configuration (verified from official docs)
```yaml
# /opt/local-ai/models/devstral-small-2.yaml
# Source: https://localai.io/advanced/model-configuration/
# + use_jinja from PR #7120 (Nov 2025)
name: devstral-small-2
backend: llama-cpp
parameters:
  model: devstral-small-2-q4km.gguf
  temperature: 0.15
  top_p: 0.95
context_size: 32768
threads: 0
mmap: true
mmlock: false
use_jinja: true
```

### systemd Unit File
```ini
# /etc/systemd/system/local-ai.service
# Source: https://localai.io/reference/cli-reference/ + community examples
[Unit]
Description=LocalAI LLM Inference Server
After=network-online.target
Wants=network-online.target

[Service]
Type=simple
User=localai
Group=localai
EnvironmentFile=/opt/local-ai/config/local-ai.env
ExecStart=/opt/local-ai/bin/local-ai run \
    --address 127.0.0.1:8080 \
    --models-path /opt/local-ai/models \
    --disable-webui
Restart=on-failure
RestartSec=10
TimeoutStartSec=300
LimitNOFILE=65536
# Prevent OOM killer from targeting this (model uses 14+ GB)
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
```

### Environment File (generated by service_configure)
```bash
# /opt/local-ai/config/local-ai.env
# Generated at boot from ONEAPP_COPILOT_* context variables
LOCALAI_THREADS=0
LOCALAI_CONTEXT_SIZE=32768
LOCALAI_LOG_LEVEL=info
```

### Health Check Loop (for service_bootstrap)
```bash
wait_for_localai() {
    local _timeout=300
    local _elapsed=0
    msg info "Waiting for LocalAI readiness (timeout: ${_timeout}s)"
    while ! curl -sf http://127.0.0.1:8080/readyz >/dev/null 2>&1; do
        sleep 5
        _elapsed=$((_elapsed + 5))
        if [ "${_elapsed}" -ge "${_timeout}" ]; then
            msg error "LocalAI not ready after ${_timeout}s -- check journalctl -u local-ai"
            exit 1
        fi
    done
    msg info "LocalAI ready (${_elapsed}s)"
}
```

### Smoke Test (for build-time verification)
```bash
# Test non-streaming chat completion
response=$(curl -sf http://127.0.0.1:8080/v1/chat/completions \
    -H 'Content-Type: application/json' \
    -d '{
        "model": "devstral-small-2",
        "messages": [{"role": "user", "content": "Write a Python hello world"}],
        "max_tokens": 50
    }')
# Verify response has content
echo "$response" | jq -e '.choices[0].message.content' >/dev/null || {
    msg error "Smoke test failed: no content in response"
    exit 1
}

# Test streaming
curl -sf http://127.0.0.1:8080/v1/chat/completions \
    -H 'Content-Type: application/json' \
    -d '{
        "model": "devstral-small-2",
        "messages": [{"role": "user", "content": "Say hello"}],
        "max_tokens": 10,
        "stream": true
    }' | grep -q 'data:' || {
    msg error "Streaming smoke test failed: no SSE data lines"
    exit 1
}
```

### ONE_SERVICE_PARAMS Array (appliance script header)
```bash
ONE_SERVICE_NAME='Service SLM-Copilot - Sovereign AI Coding Assistant'
ONE_SERVICE_VERSION='1.0.0'
ONE_SERVICE_BUILD=$(date +%s)
ONE_SERVICE_SHORT_DESCRIPTION='CPU-only AI coding copilot (Devstral Small 2 24B)'
ONE_SERVICE_DESCRIPTION='Sovereign AI coding assistant serving Devstral Small 2 24B
via LocalAI. OpenAI-compatible API for Cline/VS Code integration.
CPU-only inference, no GPU required.'
ONE_SERVICE_RECONFIGURABLE=true

ONE_SERVICE_PARAMS=(
    'ONEAPP_COPILOT_CONTEXT_SIZE'  'configure' 'Model context window in tokens (default 32768)'  '32768'
    'ONEAPP_COPILOT_THREADS'       'configure' 'CPU threads for inference (0=auto-detect)'        '0'
)
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Backends bundled in LocalAI binary | Backends externalized, auto-downloaded or CLI-installed | Mid-2025 (v3.x) | Must pre-install llama-cpp backend during build |
| Manual chat template in model YAML | `use_jinja: true` reads template from GGUF metadata | Nov 2025 (PR #7120) | Eliminates manual template maintenance; critical for Devstral |
| `install.sh` script recommended | Direct binary download from GitHub releases | Jan 2026 (#8032) | Install script is broken; use binary directly |
| `--single-active-backend` flag | `--max-active-backends=N` | v3.x | Old flag deprecated; use new flag (N=1 for single model) |
| `--watchdog-idle` / `--watchdog-busy` boolean | `--enable-watchdog-idle` / `--enable-watchdog-busy` | v3.x | Flag names changed |

**Deprecated/outdated:**
- `install.sh`: Broken since Jan 2026 (issue #8032). Use direct binary download.
- `backend: llama`: Does not exist. Use `backend: llama-cpp`.
- `--single-active-backend`: Replaced by `--max-active-backends`.
- Manual chat templates for GGUF models: Use `use_jinja: true` instead.

## LocalAI CLI Reference (Phase 1 relevant flags)

| Flag | Env Var | Default | Description |
|------|---------|---------|-------------|
| `--address` | `LOCALAI_ADDRESS` | `:8080` | Listen address. Use `127.0.0.1:8080` for loopback-only |
| `--threads` | `LOCALAI_THREADS` | (CPU count) | CPU threads for inference |
| `--context-size` | `LOCALAI_CONTEXT_SIZE` | (model YAML) | Global context size override |
| `--models-path` | `LOCALAI_MODELS_PATH` | `./models` | Directory containing model YAML + GGUF files |
| `--disable-webui` | `LOCALAI_DISABLE_WEBUI` | `false` | Disable admin WebUI. Always set to true. |
| `--preload-models` | `LOCALAI_PRELOAD_MODELS` | (none) | JSON array of models to preload at startup |
| `--enable-watchdog-idle` | `LOCALAI_WATCHDOG_IDLE` | `false` | Enable idle watchdog (unload idle models) |
| `--watchdog-idle-timeout` | `LOCALAI_WATCHDOG_IDLE_TIMEOUT` | `15m` | Idle timeout before model unload |
| `--log-level` | `LOCALAI_LOG_LEVEL` | `info` | Log verbosity |
| `--max-active-backends` | `LOCALAI_MAX_ACTIVE_BACKENDS` | `0` (unlimited) | Max concurrent backend processes |
| `--cors` | `LOCALAI_CORS` | `false` | Enable CORS (we use Nginx instead) |
| `--api-keys` | `LOCALAI_API_KEY` | (none) | API key auth (we use Nginx basic auth instead) |

**Note:** Environment variables take precedence over CLI flags.

## Context Variable to LocalAI Mapping

| ONEAPP Variable | Maps To | Where Applied |
|-----------------|---------|---------------|
| `ONEAPP_COPILOT_CONTEXT_SIZE` | `LOCALAI_CONTEXT_SIZE` in env file + `context_size` in model YAML | Both (env file for global override, YAML for model default) |
| `ONEAPP_COPILOT_THREADS` | `LOCALAI_THREADS` in env file + `threads` in model YAML | Both |

The `service_configure()` function reads `ONEAPP_COPILOT_*` variables and writes them to both the model YAML and the environment file. The environment file values take precedence at runtime.

## GGUF Model Details

| Property | Value |
|----------|-------|
| Repository | bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF |
| Filename | `mistralai_Devstral-Small-2-24B-Instruct-2512-Q4_K_M.gguf` |
| File size | 14.33 GB |
| Architecture | mistral3 |
| Parameters | 24B |
| Native context | 128K tokens |
| Recommended context (32 GB VM) | 32768 tokens |
| License | Apache 2.0 |
| Download URL | `https://huggingface.co/bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF/resolve/main/mistralai_Devstral-Small-2-24B-Instruct-2512-Q4_K_M.gguf` |

**Note on filename:** The bartowski GGUF filename includes the full path prefix `mistralai_Devstral-Small-2-24B-Instruct-2512-Q4_K_M.gguf`. When saved to the models directory, rename to a shorter name (e.g., `devstral-small-2-q4km.gguf`) and reference the short name in the model YAML.

## Naming Convention Note

The REQUIREMENTS.md specifies `ONEAPP_COPILOT_CONTEXT_SIZE` and `ONEAPP_COPILOT_THREADS`. The earlier STACK.md research used `ONEAPP_SLM_*` prefix. **Use `ONEAPP_COPILOT_*` per requirements** -- this aligns with the product name "SLM-Copilot" and is more descriptive.

## Open Questions

1. **`local-ai backends install llama-cpp` exact behavior with binary install**
   - What we know: The CLI subcommand exists and is documented. It downloads the appropriate backend binary.
   - What's unclear: Exact directory it installs to when running from a binary (vs Docker). Likely `/tmp/localai/backend_data/backend-assets/grpc/`.
   - Recommendation: Test during implementation. Run the command, verify with `local-ai backends list`, and check the filesystem for the installed backend.

2. **Environment variable vs YAML vs CLI flag precedence**
   - What we know: Official docs say "environment variables take precedence over CLI flags."
   - What's unclear: How model YAML `context_size` interacts with env `LOCALAI_CONTEXT_SIZE` -- does global env override per-model YAML?
   - Recommendation: Set both to the same value. The env file is the "runtime override" path; the YAML is the "model default."

3. **`use_jinja: true` with Devstral Small 2 specifically**
   - What we know: Feature merged Nov 2025 (PR #7120). Available in v3.11.0. llama.cpp has embedded Jinja support.
   - What's unclear: Whether Devstral Small 2's GGUF specifically includes the chat template in metadata (most bartowski GGUFs do).
   - Recommendation: Verify during pre-warm test. If chat template isn't in GGUF metadata, fall back to explicit template in YAML. The pre-warm test will catch this.

## Sources

### Primary (HIGH confidence)
- [LocalAI CLI Reference](https://localai.io/reference/cli-reference/) -- all flags, env vars, subcommands
- [LocalAI Model Configuration](https://localai.io/advanced/model-configuration/) -- YAML schema, context_size default 512, template config
- [LocalAI Backends](https://localai.io/backends/) -- backend architecture, LOCALAI_EXTERNAL_BACKENDS, gallery
- [LocalAI Binaries](https://localai.io/reference/binaries/) -- binary download URL pattern, `local-ai-Linux-x86_64`
- [LocalAI GitHub Releases v3.11.0](https://github.com/mudler/LocalAI/releases) -- Feb 7 2026, latest stable
- [LocalAI Text Generation](https://localai.io/features/text-generation/) -- use_tokenizer_template, use_jinja config
- [LocalAI PR #7120](https://github.com/mudler/LocalAI/issues/7115) -- Jinja template from GGUF metadata, merged Nov 2025
- [bartowski Devstral GGUF](https://huggingface.co/bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF) -- Q4_K_M = 14.33 GB, architecture mistral3
- [Flower SuperLink appliance script](file:///home/pablo/flower-opennebula/appliances/flower_service/appliance-superlink.sh) -- proven one-apps lifecycle pattern

### Secondary (MEDIUM confidence)
- [LocalAI Install Script Issue #8032](https://github.com/mudler/LocalAI/issues/8032) -- install.sh broken Jan 2026
- [LocalAI Backend Pre-install Issue #6174](https://github.com/mudler/LocalAI/issues/6174) -- llama-cpp not bundled in workers
- [LocalAI Preload Models Discussion #559](https://github.com/mudler/LocalAI/discussions/559) -- PRELOAD_MODELS env var
- [LocalAI Health Check Issue #1566](https://github.com/mudler/LocalAI/issues/1566) -- /readyz endpoint behavior
- [Devstral Small 2 Chat Template](https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512/blob/main/chat_template.jinja) -- official Jinja template
- [llama.cpp Devstral tool-call Issue #17960](https://github.com/ggml-org/llama.cpp/issues/17960) -- tool calling issues (closed, not relevant for basic chat)
- [Systemd for LocalAI Issue #762](https://github.com/mudler/LocalAI/issues/762) -- community systemd unit examples

### Tertiary (LOW confidence)
- LocalAI `local-ai backends install llama-cpp` exact behavior with binary install -- documented but not personally verified

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- versions, URLs, and capabilities verified against official docs and GitHub releases
- Architecture: HIGH -- patterns proven in flower-opennebula appliance + official LocalAI docs
- Pitfalls: HIGH -- combination of official docs, verified GitHub issues, and author's direct OpenNebula appliance experience
- Model YAML config: HIGH -- verified against official model configuration docs, `use_jinja` confirmed via PR #7120
- Backend pre-install: MEDIUM -- `local-ai backends install` CLI documented but exact binary-mode behavior not personally tested

**Research date:** 2026-02-14
**Valid until:** 2026-03-14 (LocalAI releases ~monthly; model GGUF is pinned)
